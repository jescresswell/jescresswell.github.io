---
title: "TabDPT: An Open Tabular Foundation Model"
collection: publications
permalink: /publication/2025-12-tabdpt
excerpt: 
date: 2025-12-02
authors: 'Junwei Ma, Valentin Thomas, Rasa Hosseinzadeh, Hamidreza Kamkari, Alex Labach, <b>Jesse C. Cresswell</b>, Keyvan Golestan, Guangwei Yu, Anthony L. Caterini, Maksims Volkovs'
note:
venueshort: 'NeurIPS 2025'
venue: 'Advances in Neural Information Processing Systems'
paperurl: 'https://arxiv.org/abs/2410.18164'
pdf: 'https://arxiv.org/pdf/2410.18164.pdf'
codeurl: 'https://github.com/layer6ai-labs/TabDPT-inference'
videourl:
slidesurl:
citation: 'Junwei Ma, Valentin Thomas, Rasa Hosseinzadeh, Hamidreza Kamkari, Alex Labach, Jesse C. Cresswell, Keyvan Golestan, Guangwei Yu, Anthony L. Caterini, Maksims Volkovs. TabDPT: An Open Tabular Foundation Model. In Advances in Neural Information Processing Systems, volume 38, 2025'
---
Tabular data is one of the most ubiquitous sources of information worldwide, spanning a wide variety of domains. This inherent heterogeneity has slowed the development of Tabular Foundation Models (TFMs) capable of fast generalization to unseen datasets. In-Context Learning (ICL) has recently emerged as a promising solution for TFMs, enabling dynamic adaptation to new tasks without additional tuning. While many studies have attempted to re-purpose large language models for tabular ICL, they have had limited success, so recent works have focused on developing tabular-specific foundation models. In this work, we propose an approach to combine ICL-based retrieval with self supervised learning to train tabular foundation models. We also investigate the utility of real vs. synthetic data for model pre-training, and show that real data can contain useful signal not easily captured in synthetic training. Specifically, we show that incorporating real data during the pre-training phase can lead to significantly faster training and better downstream generalization to unseen data. Our resulting model, TabDPT, achieves top performance on both regression (CTR23) and classification (CC18) benchmarks. Importantly, we also demonstrate that with our pre-training procedure, scaling both model and data size leads to consistent performance improvements that follow power laws. This echoes scaling laws in LLMs and other foundation models, and suggests that Internet-scale TFMs can be achievable.